{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4f260b-7873-46bc-b994-a3b766a917be",
   "metadata": {},
   "source": [
    "# Library Imports\n",
    "\n",
    "We import all the necessary Python libraries used throughout the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab37c3c4-08fb-44a8-86af-75da7e5d0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import feedparser\n",
    "import json\n",
    "from urllib.parse import quote_plus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b701299-a3e7-4578-afdb-c9d8b1716752",
   "metadata": {},
   "source": [
    "The file aims to extract abstracts of papers from arXiv. In particular, we focus on papers with a literary theme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d35c5f88-8876-4a6c-b62d-50151465ae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting arXiv fetch...\n",
      "arXiv request: start=0 max_results=100\n",
      "Found 100 entries, collected so far: 9 / 1000\n",
      "arXiv request: start=100 max_results=100\n",
      "Found 100 entries, collected so far: 21 / 1000\n",
      "arXiv request: start=200 max_results=100\n",
      "Found 100 entries, collected so far: 47 / 1000\n",
      "arXiv request: start=300 max_results=100\n",
      "Found 100 entries, collected so far: 90 / 1000\n",
      "arXiv request: start=400 max_results=100\n",
      "Found 100 entries, collected so far: 118 / 1000\n",
      "arXiv request: start=500 max_results=100\n",
      "Found 100 entries, collected so far: 149 / 1000\n",
      "arXiv request: start=600 max_results=100\n",
      "Found 100 entries, collected so far: 182 / 1000\n",
      "arXiv request: start=700 max_results=100\n",
      "Found 100 entries, collected so far: 258 / 1000\n",
      "arXiv request: start=800 max_results=100\n",
      "Found 100 entries, collected so far: 328 / 1000\n",
      "arXiv request: start=900 max_results=100\n",
      "Found 100 entries, collected so far: 377 / 1000\n",
      "arXiv request: start=1000 max_results=100\n",
      "Found 100 entries, collected so far: 435 / 1000\n",
      "arXiv request: start=1100 max_results=100\n",
      "No more entries found: stopping\n",
      "Downloaded 435 abstracts. Saving to file...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dict contains fields not in fieldnames: 'primary_category'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 141\u001b[0m\n\u001b[1;32m    139\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriteheader()\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(abstracts, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 141\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAuthors\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauthors\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprimary_category\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprimary_category\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCategories\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategories\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone. Files: abstracts.json  abstracts.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/csv.py:154\u001b[0m, in \u001b[0;36mDictWriter.writerow\u001b[0;34m(self, rowdict)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwriterow\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdict):\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mwriterow(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrowdict\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/csv.py:149\u001b[0m, in \u001b[0;36mDictWriter._dict_to_list\u001b[0;34m(self, rowdict)\u001b[0m\n\u001b[1;32m    147\u001b[0m     wrong_fields \u001b[38;5;241m=\u001b[39m rowdict\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrong_fields:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict contains fields not in fieldnames: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m                          \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m wrong_fields]))\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (rowdict\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestval) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames)\n",
      "\u001b[0;31mValueError\u001b[0m: dict contains fields not in fieldnames: 'primary_category'"
     ]
    }
   ],
   "source": [
    "KEYWORDS = [\n",
    "    \"literature\",\n",
    "    \"literary\",\n",
    "    \"philology\",\n",
    "    \"book history\",\n",
    "    \"textual criticism\",\n",
    "    \"textual criticism\",\n",
    "    \"book-history\",\n",
    "    \"book_history\",\n",
    "    \"digital humanities\",\n",
    "    \"book studies\",\n",
    "    \"manuscript\",\n",
    "    \"codicology\",\n",
    "    \"paleography\",\n",
    "    \"literary theory\",\n",
    "    \"comparative literature\",\n",
    "    \"literary criticism\",\n",
    "]\n",
    "\n",
    "TARGET_COUNT = 1000  \n",
    "BATCH_SIZE = 100     \n",
    "BASE_URL = \"http://export.arxiv.org/api/query?\"\n",
    "\n",
    "# Utility\n",
    "def make_query(keywords):\n",
    "    parts = []\n",
    "    for kw in keywords:\n",
    "        parts.append('all:%s' % quote_plus('\"%s\"' % kw))\n",
    "    return '+OR+'.join(parts)\n",
    "\n",
    "\n",
    "def parse_entry(entry):\n",
    "    arxiv_id = entry.get('id')\n",
    "    title = entry.get('title', '').strip().replace('\\n', ' ')\n",
    "    abstract = entry.get('summary', '').strip().replace('\\n', ' ')\n",
    "    authors = [a.name for a in entry.get('authors', [])] if entry.get('authors') else []\n",
    "    # categories: feedparser puts tags in 'term'\n",
    "    cats = [t['term'] for t in entry.get('tags', [])] if entry.get('tags') else []\n",
    "    primary_cat = cats[0] if cats else None\n",
    "    # pdf link\n",
    "    pdf_url = None\n",
    "    for link in entry.get('links', []):\n",
    "        if link.get('type') == 'application/pdf':\n",
    "            pdf_url = link.get('href')\n",
    "            break\n",
    "    return {\n",
    "        'id': arxiv_id,\n",
    "        'title': title,\n",
    "        'authors': authors,\n",
    "        'primary_category': primary_cat,\n",
    "        'categories': cats,\n",
    "        'abstract': abstract,\n",
    "        'pdf_url': pdf_url,\n",
    "    }\n",
    "\n",
    "\n",
    "def is_cs_paper(parsed_entry):\n",
    "    pc = parsed_entry.get('primary_category')\n",
    "    if not pc:\n",
    "        return False\n",
    "    return pc.startswith('cs')\n",
    "\n",
    "\n",
    "def fetch_arxiv_abstracts(keywords, target_count=1000, batch_size=100, pause_seconds=3):\n",
    "    collected = []\n",
    "    seen_ids = set()\n",
    "    query = make_query(keywords)\n",
    "    start = 0\n",
    "    total_results_estimate = None\n",
    "\n",
    "    while len(collected) < target_count:\n",
    "        url = f\"{BASE_URL}search_query={query}&start={start}&max_results={batch_size}\"\n",
    "        print(f\"arXiv request: start={start} max_results={batch_size}\")\n",
    "        r = requests.get(url, headers={'User-Agent': 'arXivAbstractFetcher/1.0 (+https://example.org)'})\n",
    "        if r.status_code != 200:\n",
    "            print(f\"HTTP error {r.status_code} - stopping\")\n",
    "            break\n",
    "        feed = feedparser.parse(r.text)\n",
    "\n",
    "        if total_results_estimate is None:\n",
    "            # feed.opensearch_totalresults usually exists\n",
    "            try:\n",
    "                total_results_estimate = int(feed['feed'].get('opensearch_totalresults', 0))\n",
    "            except Exception:\n",
    "                total_results_estimate = None\n",
    "\n",
    "        entries = feed.entries\n",
    "        if not entries:\n",
    "            print(\"No more entries found: stopping\")\n",
    "            break\n",
    "\n",
    "        new_found = 0\n",
    "        for e in entries:\n",
    "            parsed = parse_entry(e)\n",
    "            # deduplication\n",
    "            if parsed['id'] in seen_ids:\n",
    "                continue\n",
    "            seen_ids.add(parsed['id'])\n",
    "            new_found += 1\n",
    "            # filter out CS papers\n",
    "            if is_cs_paper(parsed):\n",
    "                continue\n",
    "            collected.append(parsed)\n",
    "            if len(collected) >= target_count:\n",
    "                break\n",
    "\n",
    "        print(f\"Found {new_found} entries, collected so far: {len(collected)} / {target_count}\")\n",
    "\n",
    "        # move forward\n",
    "        start += batch_size\n",
    "\n",
    "        # if estimated total is reached, stop\n",
    "        if total_results_estimate is not None and start >= total_results_estimate:\n",
    "            print(\"Reached the estimated total number of results from the arXiv API.\")\n",
    "            break\n",
    "\n",
    "        # pause to respect rate limits\n",
    "        time.sleep(pause_seconds)\n",
    "\n",
    "    return collected\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Starting arXiv fetch...\")\n",
    "    abstracts = fetch_arxiv_abstracts(KEYWORDS, target_count=TARGET_COUNT, batch_size=BATCH_SIZE)\n",
    "    print(f\"Downloaded {len(abstracts)} abstracts. Saving to file...\")\n",
    "\n",
    "    # Save JSON\n",
    "    with open('abstracts.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(abstracts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    import csv\n",
    "    \n",
    "    # Save abstracts in CSV\n",
    "    with open('../Datasets/abstracts.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Index', 'Title', 'Authors', 'Categories', 'abstract']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "        writer.writeheader()\n",
    "        for i, a in enumerate(abstracts, 1):\n",
    "            writer.writerow({\n",
    "                'Title': a['title'],\n",
    "                'Authors': ', '.join(a['authors']),\n",
    "                'primary_category': \",\".join(a['primary_category']),\n",
    "                'Categories': ', '.join(a['categories']),\n",
    "                'abstract': a['abstract'].strip()\n",
    "            })\n",
    "\n",
    "    print(\"Done. Files: abstracts.json  abstracts.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3af77-183e-4f01-ad2c-48a7da31f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import feedparser\n",
    "from urllib.parse import quote_plus\n",
    "import csv\n",
    "\n",
    "KEYWORDS = [\n",
    "    \"literature\",\n",
    "    \"literary\",\n",
    "    \"philology\",\n",
    "    \"book history\",\n",
    "    \"textual criticism\",\n",
    "    \"book-history\",\n",
    "    \"book_history\",\n",
    "    \"digital humanities\",\n",
    "    \"book studies\",\n",
    "    \"manuscript\",\n",
    "    \"codicology\",\n",
    "    \"paleography\",\n",
    "    \"literary theory\",\n",
    "    \"comparative literature\",\n",
    "    \"literary criticism\",\n",
    "]\n",
    "\n",
    "TARGET_COUNT = 1000\n",
    "BATCH_SIZE = 100\n",
    "BASE_URL = \"http://export.arxiv.org/api/query?\"\n",
    "\n",
    "\n",
    "# Utility\n",
    "def make_query(keywords):\n",
    "    parts = []\n",
    "    for kw in keywords:\n",
    "        parts.append('all:%s' % quote_plus('\"%s\"' % kw))\n",
    "    return '+OR+'.join(parts)\n",
    "\n",
    "\n",
    "def parse_entry(entry):\n",
    "    arxiv_id = entry.get('id')\n",
    "    title = entry.get('title', '').strip().replace('\\n', ' ')\n",
    "    abstract = entry.get('summary', '').strip().replace('\\n', ' ')\n",
    "    authors = [a.name for a in entry.get('authors', [])] if entry.get('authors') else []\n",
    "    cats = [t['term'] for t in entry.get('tags', [])] if entry.get('tags') else []\n",
    "    primary_cat = cats[0] if cats else None\n",
    "    return {\n",
    "        'id': arxiv_id,\n",
    "        'title': title,\n",
    "        'authors': authors,\n",
    "        'primary_category': primary_cat,\n",
    "        'categories': cats,\n",
    "        'abstract': abstract,\n",
    "    }\n",
    "\n",
    "\n",
    "def is_cs_paper(parsed_entry):\n",
    "    pc = parsed_entry.get('primary_category')\n",
    "    return pc.startswith('cs') if pc else False\n",
    "\n",
    "\n",
    "def fetch_arxiv_abstracts(keywords, target_count=1000, batch_size=100, pause_seconds=3):\n",
    "    collected = []\n",
    "    seen_ids = set()\n",
    "    query = make_query(keywords)\n",
    "    start = 0\n",
    "    total_results_estimate = None\n",
    "\n",
    "    while len(collected) < target_count:\n",
    "        url = f\"{BASE_URL}search_query={query}&start={start}&max_results={batch_size}\"\n",
    "        print(f\"arXiv request: start={start} max_results={batch_size}\")\n",
    "        r = requests.get(url, headers={'User-Agent': 'arXivAbstractFetcher/1.0'})\n",
    "        if r.status_code != 200:\n",
    "            print(f\"HTTP error {r.status_code} - stopping\")\n",
    "            break\n",
    "\n",
    "        feed = feedparser.parse(r.text)\n",
    "        if total_results_estimate is None:\n",
    "            try:\n",
    "                total_results_estimate = int(feed['feed'].get('opensearch_totalresults', 0))\n",
    "            except Exception:\n",
    "                total_results_estimate = None\n",
    "\n",
    "        entries = feed.entries\n",
    "        if not entries:\n",
    "            print(\"No more entries found: stopping\")\n",
    "            break\n",
    "\n",
    "        new_found = 0\n",
    "        for e in entries:\n",
    "            parsed = parse_entry(e)\n",
    "            if parsed['id'] in seen_ids or is_cs_paper(parsed):\n",
    "                continue\n",
    "            seen_ids.add(parsed['id'])\n",
    "            collected.append(parsed)\n",
    "            new_found += 1\n",
    "            if len(collected) >= target_count:\n",
    "                break\n",
    "\n",
    "        print(f\"Found {new_found} new entries, collected so far: {len(collected)} / {target_count}\")\n",
    "\n",
    "        start += batch_size\n",
    "        if total_results_estimate is not None and start >= total_results_estimate:\n",
    "            print(\"Reached the estimated total number of results from the arXiv API.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(pause_seconds)\n",
    "\n",
    "    return collected\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Starting arXiv fetch...\")\n",
    "    abstracts = fetch_arxiv_abstracts(KEYWORDS, target_count=TARGET_COUNT, batch_size=BATCH_SIZE)\n",
    "    print(f\"Downloaded {len(abstracts)} abstracts. Saving to CSV...\")\n",
    "\n",
    "    with open('abstracts.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Index', 'Title', 'Authors', 'Categories', 'Abstract']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i, a in enumerate(abstracts, 1):\n",
    "            writer.writerow({\n",
    "                'Index': i,\n",
    "                'Title': a['title'],\n",
    "                'Authors': ', '.join(a['authors']),\n",
    "                'Categories': ', '.join(a['categories']),\n",
    "                'Abstract': a['abstract'].strip()\n",
    "            })\n",
    "\n",
    "    print(\"Done. File created: abstracts.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
