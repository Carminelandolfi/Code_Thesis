{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e647a6e-8721-4703-8057-29472a86d0df",
   "metadata": {},
   "source": [
    "## 1. Library Imports\n",
    "\n",
    "\n",
    "In this section, we import all the necessary libraries for data preprocessing, tokenization, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9117a4a0-340d-4b42-a6cb-97f9a9d53f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/carmine-\n",
      "[nltk_data]     landolfi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "\n",
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf642ab-532b-4be3-be8b-1910a97f4b46",
   "metadata": {},
   "source": [
    "# 2. Dataset Loading\n",
    "\n",
    "In this section, we load the dataset of scientific abstracts that were previously collected and cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a731f7f7-6f5c-4e01-afd0-7dc4a6fb788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/cleaned_dataset.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce78e5b-faff-4ad6-873a-0075ac994c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_del = pd.read_csv(\"../Datasets/Index_del.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec051ac-9db6-4741-9594-68a8ef9126a2",
   "metadata": {},
   "source": [
    "# Dataset Creation\n",
    "\n",
    "The following datasets have been prepared for various stages of the experimental pipeline:\n",
    "\n",
    "- **Domain-Adaptive Pretraining (DAPT):**  \n",
    "  Creation of three datasets consisting of:\n",
    "  - 10K abstracts  \n",
    "  - 50K abstracts  \n",
    "  - 100K abstracts  \n",
    "\n",
    "- **Masked Language Modeling (MLM) Evaluation:**  \n",
    "  A separate dataset of 20K abstracts, not overlapping with the DAPT datasets, used to evaluate the model's performance on the MLM task.\n",
    "\n",
    "- **Fine-Tuning:**  \n",
    "  A residual dataset, composed of all remaining abstracts not used in DAPT or MLM evaluation, reserved for downstream fine-tuning tasks (e.g., classification).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbafd2af-b92c-4406-95fe-6b4fc2a6ba4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1355395/801149295.py:21: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  indices_10k = set(random.sample(indices_100k, SAMPLE_10K))\n",
      "/tmp/ipykernel_1355395/801149295.py:24: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  indices_50k = set(random.sample(indices_100k, SAMPLE_50K))\n",
      "/tmp/ipykernel_1355395/801149295.py:28: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  indices_10k_val = set(random.sample(remaining_after_100k, VAL_SAMPLE_SIZE))\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "DAPT_SAMPLE_SIZE = 100_000      # Total abstracts to sample for main dataset\n",
    "SAMPLE_10K = 10_000             # Size of smaller subset from 100K\n",
    "SAMPLE_50K = 50_000             # Size of larger subset from 100K \n",
    "VAL_SAMPLE_SIZE = 20_000        # Validation dataset size from outside the 100K\n",
    "SEED = 123                     # Random seed for reproducibility\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "indices_to_delete = index_del['indices']\n",
    "\n",
    "# delete indeces from df\n",
    "df = df.drop(index=indices_to_delete, errors='ignore').reset_index()\n",
    "N = len(df)  # Total number of abstracts available\n",
    "\n",
    "# Step 1: Randomly sample 100K abstracts for the main domain-adaptive pretraining dataset\n",
    "indices_100k = set(random.sample(range(N), DAPT_SAMPLE_SIZE))\n",
    "\n",
    "# Step 2: From the 100K dataset, randomly select 10K abstracts\n",
    "indices_10k = set(random.sample(indices_100k, SAMPLE_10K))\n",
    "\n",
    "# Step 3: From the remaining 90K (100K minus 10K), select 50K abstracts\n",
    "indices_50k = set(random.sample(indices_100k, SAMPLE_50K))\n",
    "\n",
    "# Step 4: From the abstracts NOT in the 100K sample, randomly select 20K for validation using MLM\n",
    "remaining_after_100k = set(range(N)) - indices_100k\n",
    "indices_10k_val = set(random.sample(remaining_after_100k, VAL_SAMPLE_SIZE))\n",
    "\n",
    "# Step 5: The remaining abstracts not selected in any of the above datasets for Fine-Tuning\n",
    "indices_remaining = set(range(N)) - indices_100k - indices_10k_val\n",
    "\n",
    "# Create the five datasets using the selected indices\n",
    "df_100k = df.loc[list(indices_100k)].reset_index()\n",
    "df_10k = df.loc[list(indices_10k)].reset_index()\n",
    "df_50k = df.loc[list(indices_50k)].reset_index()\n",
    "df_10k_val = df.loc[list(indices_10k_val)].reset_index()\n",
    "df_remaining = df.loc[list(indices_remaining)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a2fddab-e5a3-4ecb-8a13-7a20af851900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerSegmenter:\n",
    "    \"\"\"\n",
    "    A utility class for segmenting and tokenizing textual abstracts into\n",
    "    overlapping BERT-compatible input chunks.\n",
    "\n",
    "    This is especially useful for handling long abstracts that exceed the BERT\n",
    "    maximum input length (typically 512 tokens). It applies a sliding window\n",
    "    approach with a defined stride to create multiple segments per abstract,\n",
    "    ensuring coverage while preserving sentence boundaries.\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer (BertTokenizer): Pretrained BERT tokenizer for tokenizing text.\n",
    "        max_length (int): Maximum sequence length for BERT input (default is 512).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: BertTokenizer, max_length: int = 512):\n",
    "        \"\"\"\n",
    "        Initializes the TokenizerSegmenter.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (BertTokenizer): The tokenizer used to encode the abstracts.\n",
    "            max_length (int): Maximum length of tokenized input sequences.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def process(self, abstracts: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Processes a list of abstracts into padded, tokenized BERT-compatible inputs\n",
    "        using a sliding window strategy.\n",
    "\n",
    "        Args:\n",
    "            abstracts (List[str]): List of raw abstract texts to be tokenized and segmented.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing:\n",
    "                - 'input_ids' (Tensor): Padded token IDs for each segment.\n",
    "                - 'attention_mask' (Tensor): Attention masks for each segment.\n",
    "                - 'token_type_ids' (Tensor): Segment token type IDs (all zeros).\n",
    "        \"\"\"\n",
    "        input_ids_all: List[List[int]] = []\n",
    "        attention_masks_all: List[List[int]] = []\n",
    "        token_type_ids_all: List[List[int]] = []\n",
    "        abstract_ids_all: List[int] = []\n",
    "        abstract_texts_all: List[str] = []\n",
    "\n",
    "        for idx, abstract in enumerate(abstracts):\n",
    "            sentences = sent_tokenize(abstract)\n",
    "            token_ids: List[int] = [self.tokenizer.cls_token_id]\n",
    "\n",
    "            # Encode each sentence and add [SEP] token\n",
    "            for sent in sentences:\n",
    "                sent_ids = self.tokenizer.encode(sent, add_special_tokens=False)\n",
    "                token_ids.extend(sent_ids + [self.tokenizer.sep_token_id])\n",
    "\n",
    "            \n",
    "            chunk = token_ids\n",
    "\n",
    "            # Ensure CLS and SEP tokens\n",
    "            if chunk[0] != self.tokenizer.cls_token_id:\n",
    "                    chunk = [self.tokenizer.cls_token_id] + chunk[:len(chunk)-1]\n",
    "            if chunk[-1] != self.tokenizer.sep_token_id:\n",
    "                    chunk[-1] = self.tokenizer.sep_token_id\n",
    "\n",
    "\n",
    "            pad_len = self.max_length - len(chunk)\n",
    "\n",
    "            chunk_padded = chunk + [self.tokenizer.pad_token_id] * pad_len\n",
    "            attention_mask = [1] * len(chunk) + [0] * pad_len\n",
    "\n",
    "            input_ids_all.append(chunk_padded)\n",
    "            attention_masks_all.append(attention_mask)\n",
    "                \n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids_all, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_masks_all, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c4c249-eac3-4061-9875-de15a2873bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForPreTraining.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "prep = TokenizerSegmenter(tokenizer,max_length = 300)\n",
    "\n",
    "prep_100k = prep.process(df_100k[\"abstract_clean\"])\n",
    "prep_10k = prep.process(df_10k[\"abstract_clean\"])\n",
    "prep_50k = prep.process(df_50k[\"abstract_clean\"])\n",
    "prep_10k_val = prep.process(df_10k_val[\"abstract_clean\"])\n",
    "prep_remaining = prep.process(df_remaining[\"abstract_clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41a4c96d-d396-4707-892b-dce5f81e3050",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100k = pd.DataFrame({\n",
    "    \"abstract_clean\": df_100k[\"abstract_clean\"],\n",
    "    \"input_ids\": prep_100k[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": prep_100k[\"attention_mask\"].tolist()\n",
    "})\n",
    "\n",
    "df_10k = pd.DataFrame({\n",
    "    \"abstract_clean\": df_10k[\"abstract_clean\"],\n",
    "    \"input_ids\": prep_10k[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": prep_10k[\"attention_mask\"].tolist()\n",
    "})\n",
    "\n",
    "df_50k = pd.DataFrame({\n",
    "    \"abstract_clean\": df_50k[\"abstract_clean\"],\n",
    "    \"input_ids\": prep_50k[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": prep_50k[\"attention_mask\"].tolist()\n",
    "    #\"token_type_ids\": prep_50k[\"token_type_ids\"].tolist()\n",
    "})\n",
    "\n",
    "df_10k_val = pd.DataFrame({\n",
    "    \"abstract_clean\": df_10k_val[\"abstract_clean\"],\n",
    "    \"input_ids\": prep_10k_val[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": prep_10k_val[\"attention_mask\"].tolist()\n",
    "})\n",
    "\n",
    "df_remaining = pd.DataFrame({\n",
    "    \"abstract_clean\": df_remaining[\"abstract_clean\"],\n",
    "    \"primary_category\": df_remaining[\"primary_category\"],\n",
    "    \"input_ids\": prep_remaining[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": prep_remaining[\"attention_mask\"].tolist()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b3efc71-9a25-42a2-b8a2-c32c7b5ca061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets to CSV files with headers\n",
    "df_100k.to_csv('../Datasets/dataset_100k.csv', header=True, index = False)\n",
    "df_10k.to_csv('../Datasets/dataset_10k.csv', header=True, index = False)\n",
    "df_50k.to_csv('../Datasets/dataset_50k.csv', header=True, index = False)\n",
    "df_10k_val.to_csv('../Datasets/dataset_10k_val.csv', header=True, index = False)\n",
    "df_remaining.to_csv('../Datasets/dataset_remaining.csv', header=True, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
