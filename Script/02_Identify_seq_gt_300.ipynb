{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e6ed79-c683-4295-9a97-ebba1390e967",
   "metadata": {},
   "source": [
    "# Library Imports\n",
    "\n",
    "We import all the necessary Python libraries used throughout the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c79c33-80c3-483f-a96b-33f5312ab92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/carmine-\n",
      "[nltk_data]     landolfi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from transformers import BertTokenizer, BertForPreTraining, PreTrainedTokenizer, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb33bcd2-e0ae-4c10-bd17-750b00beece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation:\n",
    "    \"\"\"\n",
    "    Prepares data for BERT-style pretraining tasks, including Masked Language Modeling (MLM), \n",
    "    from a dataset of text abstracts.\n",
    "\n",
    "    Attributes:\n",
    "        df (pd.DataFrame): DataFrame containing abstracts with a column 'abstract_clean'.\n",
    "        tokenizer (PreTrainedTokenizer): HuggingFace tokenizer compatible with BERT.\n",
    "        max_length (int): Maximum token length for BERT inputs.\n",
    "        mask_prob (float): Probability of masking tokens for MLM\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe: pd.DataFrame, tokenizer: PreTrainedTokenizer,\n",
    "                 max_length: int = 512, mask_prob: float = 0.15):\n",
    "        self.df = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mask_prob = mask_prob\n",
    "    \n",
    "\n",
    "    def tokenize_and_segment_abstracts(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Segments abstracts into overlapping tokenized chunks for MLM training.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing:\n",
    "                - input_ids (torch.Tensor): Tensor of token IDs with shape (num_segments, Max_length).\n",
    "                - attention_mask (torch.Tensor): Attention masks for each segment.\n",
    "                - token_type_ids (torch.Tensor): Token type IDs for each segment.\n",
    "        \"\"\"\n",
    "        input_ids_all, attention_masks_all, token_type_ids_all = [], [], []\n",
    "\n",
    "        for abstract in self.df[\"abstract_clean\"]:\n",
    "            sentences = sent_tokenize(abstract)\n",
    "            token_ids = [self.tokenizer.cls_token_id]\n",
    "\n",
    "            for sent in sentences:\n",
    "                sent_ids = self.tokenizer.encode(sent, add_special_tokens=False)\n",
    "                token_ids.extend(sent_ids + [self.tokenizer.sep_token_id])\n",
    "                \n",
    "            chunk = token_ids[: self.max_length]\n",
    "            len_chunk = len(chunk)\n",
    "                \n",
    "            # Ensure CLS is at the beginning\n",
    "            if chunk[0] != self.tokenizer.cls_token_id:\n",
    "                    chunk = [self.tokenizer.cls_token_id] + chunk[:(len_chunk-1)]\n",
    "            # Ensure SEP is at the end\n",
    "            if chunk[-1] != self.tokenizer.sep_token_id:\n",
    "                    chunk[-1]= self.tokenizer.sep_token_id\n",
    "            pad_len = self.max_length - len_chunk\n",
    "\n",
    "            chunk_padded = chunk + [self.tokenizer.pad_token_id] * pad_len\n",
    "            attention_mask = [1] * len_chunk + [0] * pad_len\n",
    "            token_type_ids = [0] * self.max_length\n",
    "\n",
    "            input_ids_all.append(chunk_padded)\n",
    "            attention_masks_all.append(attention_mask)\n",
    "            token_type_ids_all.append(token_type_ids)\n",
    "                \n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids_all, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_masks_all, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids_all, dtype=torch.long),\n",
    "        }\n",
    "        \n",
    "    def tokenize_and_segment_abstracts_2(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Segments abstracts into 512-token chunks for MLM training, and tracks how many segments\n",
    "        each abstract was split into (for downstream use or tracking).\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing:\n",
    "                - input_ids (torch.Tensor)\n",
    "                - attention_mask (torch.Tensor)\n",
    "                - token_type_ids (torch.Tensor)\n",
    "                - segment_index (torch.Tensor): 0 for first segment, 1 for second, etc.\n",
    "        \"\"\"\n",
    "        input_ids_all, attention_masks_all, token_type_ids_all, segment_index_all = [], [], [], []\n",
    "        stride = 50  \n",
    "    \n",
    "        for abstract in self.df[\"abstract_clean\"]:\n",
    "            sentences = sent_tokenize(abstract)\n",
    "            token_ids = [self.tokenizer.cls_token_id]\n",
    "    \n",
    "            for sent in sentences:\n",
    "                sent_ids = self.tokenizer.encode(sent, add_special_tokens=False)\n",
    "                token_ids.extend(sent_ids + [self.tokenizer.sep_token_id])\n",
    "    \n",
    "            segment_count = 0\n",
    "            start = 0\n",
    "            while start < len(token_ids):\n",
    "                chunk = token_ids[start:start + self.max_length]\n",
    "                len_chunk = len(chunk)\n",
    "    \n",
    "                # Ensure CLS at start and SEP at end\n",
    "                if chunk[0] != self.tokenizer.cls_token_id:\n",
    "                    chunk = [self.tokenizer.cls_token_id] + chunk[:self.max_length - 1]\n",
    "                if chunk[-1] != self.tokenizer.sep_token_id:\n",
    "                    if len(chunk) == self.max_length:\n",
    "                        chunk[-1] = self.tokenizer.sep_token_id\n",
    "                    else:\n",
    "                        chunk.append(self.tokenizer.sep_token_id)\n",
    "    \n",
    "                # Pad to max_length\n",
    "                chunk = chunk[:self.max_length]\n",
    "                pad_len = self.max_length - len(chunk)\n",
    "                chunk_padded = chunk + [self.tokenizer.pad_token_id] * pad_len\n",
    "                attention_mask = [1] * len(chunk) + [0] * pad_len\n",
    "                token_type_ids = [0] * self.max_length\n",
    "    \n",
    "                input_ids_all.append(chunk_padded)\n",
    "                attention_masks_all.append(attention_mask)\n",
    "                token_type_ids_all.append(token_type_ids)\n",
    "                segment_index_all.append(segment_count)\n",
    "    \n",
    "                segment_count += 1\n",
    "                start += self.max_length - stride  # move forward with stride\n",
    "    \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids_all, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_masks_all, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids_all, dtype=torch.long),\n",
    "            \"segment_index\": torch.tensor(segment_index_all, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e446107-13f5-471d-a312-4180e438f280",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForPreTraining\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m prep \u001b[38;5;241m=\u001b[39m DataPreparation(df, tokenizer)\n\u001b[0;32m----> 9\u001b[0m mlm_encodings \u001b[38;5;241m=\u001b[39m \u001b[43mprep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_and_segment_abstracts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m, in \u001b[0;36mDataPreparation.tokenize_and_segment_abstracts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mcls_token_id]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m---> 37\u001b[0m     sent_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     token_ids\u001b[38;5;241m.\u001b[39mextend(sent_ids \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msep_token_id])\n\u001b[1;32m     40\u001b[0m chunk \u001b[38;5;241m=\u001b[39m token_ids[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length]\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2649\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2611\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2612\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2613\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2635\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2647\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2648\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2649\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2659\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2660\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3040\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3011\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[1;32m   3013\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[38;5;124;03m        method).\u001b[39;00m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3031\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3032\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3033\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3037\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3038\u001b[0m )\n\u001b[0;32m-> 3040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3043\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3059\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3061\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/transformers/tokenization_utils.py:800\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    798\u001b[0m     )\n\u001b[0;32m--> 800\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    804\u001b[0m     first_ids,\n\u001b[1;32m    805\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    821\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/transformers/tokenization_utils.py:767\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 767\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/transformers/tokenization_utils.py:697\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 697\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    698\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py:161\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[0;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[1;32m    159\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[1;32m    166\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py:329\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    327\u001b[0m     token \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrip_accents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 329\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_strip_accents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrip_accents:\n\u001b[1;32m    331\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_strip_accents(token)\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py:342\u001b[0m, in \u001b[0;36mBasicTokenizer._run_strip_accents\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    340\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[0;32m--> 342\u001b[0m     cat \u001b[38;5;241m=\u001b[39m \u001b[43municodedata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cat \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Datasets/cleaned_dataset.csv\")\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForPreTraining.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "prep = DataPreparation(df, tokenizer)\n",
    "mlm_encodings = prep.tokenize_and_segment_abstracts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a70abdb-568c-4efc-a21e-83663474eb82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_ab 61465\n",
      "Num_ab 31601\n"
     ]
    }
   ],
   "source": [
    "# Count the non-zero values per row\n",
    "non_zero_per_row = (mlm_encodings[\"input_ids\"] != 0).sum(dim=1)\n",
    "lista = []\n",
    "\n",
    "# Store the result in a list\n",
    "for i, count in enumerate(non_zero_per_row):\n",
    "    lista.append(count.item())\n",
    "\n",
    "# Print how many rows have more than 256 non-zero values\n",
    "print(\"Num_ab\", len([x for x in lista if x > 256]))\n",
    "\n",
    "# Print how many rows have more than 300 non-zero values\n",
    "print(\"Num_ab\", len([x for x in lista if x > 300]))\n",
    "\n",
    "# Find the indices where the value is greater than 300\n",
    "indices = [i for i, x in enumerate(lista) if x > 300]\n",
    "\n",
    "# save the indices to a CSV file\n",
    "\n",
    "df_index = pd.DataFrame(indices, columns=['indices'])\n",
    "#df_index.to_csv('../Datasets/Index_del.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a40d3-5d85-4f14-936d-78a33b09706d",
   "metadata": {},
   "source": [
    "## Real length abstract\n",
    "\n",
    "This part allows to identify the real len of the abstract and it is useful to create a boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2abc0c3-3b44-482c-80fb-70343d22d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_encodings_2 = prep.tokenize_and_segment_abstracts_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7bb7798-d88d-46b0-83af-61e0b4dc6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_original_sequences(encodings: Dict[str, torch.Tensor]) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Reconstructs the original token sequences (input_ids) from overlapping segments created with a stride.\n",
    "\n",
    "    Args:\n",
    "        encodings (dict): Output from the `tokenize_and_segment_abstracts_2()` function. \n",
    "                          Should contain \"input_ids\" and \"segment_index\" as keys.\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: A list of reconstructed token sequences, one per abstract.\n",
    "    \"\"\"\n",
    "    input_ids = encodings[\"input_ids\"]\n",
    "    segment_indices = encodings[\"segment_index\"]\n",
    "\n",
    "    reconstructed_sequences = []\n",
    "    current_sequence = []\n",
    "    last_segment_id = -1\n",
    "    stride = 50\n",
    "    max_len = 512\n",
    "\n",
    "    for i, seg_id in enumerate(segment_indices):\n",
    "        chunk = input_ids[i].tolist()\n",
    "\n",
    "        # First segment of a new abstract\n",
    "        if seg_id == 0:\n",
    "            if current_sequence:\n",
    "                reconstructed_sequences.append(current_sequence)\n",
    "            current_sequence = chunk\n",
    "        else:\n",
    "            # Only add new tokens (excluding the overlapping stride)\n",
    "            new_tokens = chunk[stride:]\n",
    "            current_sequence += new_tokens\n",
    "\n",
    "        last_segment_id = seg_id\n",
    "\n",
    "    # Append the last reconstructed sequence\n",
    "    if current_sequence:\n",
    "        reconstructed_sequences.append(current_sequence)\n",
    "\n",
    "    return reconstructed_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f2f3b67-9c4d-4a45-8bbc-c4fc7596d691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    200094.000000\n",
      "mean        218.001234\n",
      "std          79.873906\n",
      "min           4.000000\n",
      "25%         162.000000\n",
      "50%         214.000000\n",
      "75%         271.000000\n",
      "max         876.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reconstructed = reconstruct_original_sequences(mlm_encodings_2)\n",
    "\n",
    "# Compute the actual length (i.e., number of tokens â‰  0) for each reconstructed sequence\n",
    "non_zero_lengths = [sum(1 for token in seq if token != 0) for seq in reconstructed]\n",
    "\n",
    "# Convert the list of lengths into a pandas Series\n",
    "non_zero_series = pd.Series(non_zero_lengths)\n",
    "\n",
    "# Print descriptive statistics of the sequence lengths\n",
    "print(non_zero_series.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5199b41c-e0b9-46f6-83da-8d321cfc90a4",
   "metadata": {},
   "source": [
    "Compute percentage of sequences exceeding token length thresholds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "884bd6ed-5e4a-4c01-b305-31c80facb80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage > 512 tokens: 0.03% (65 sequences)\n",
      "Percentage > 300 tokens: 15.79% (31601 sequences)\n",
      "Percentage > 256 tokens: 30.72% (61465 sequences)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Total number of reconstructed sequences\n",
    "total = len(non_zero_lengths)\n",
    "\n",
    "# Compute the percentage of sequences longer than 512 tokens\n",
    "percent_over_512 = sum(l > 512 for l in non_zero_lengths) / total * 100\n",
    "\n",
    "# Compute the percentage of sequences longer than 300 tokens\n",
    "percent_over_300 = sum(l > 300 for l in non_zero_lengths) / total * 100\n",
    "\n",
    "# Compute the percentage of sequences longer than 256 tokens\n",
    "percent_over_256 = sum(l > 256 for l in non_zero_lengths) / total * 100\n",
    "\n",
    "# Print the results with percentage and absolute count\n",
    "print(f\"Percentage > 512 tokens: {percent_over_512:.2f}% ({sum(l > 512 for l in non_zero_lengths)} sequences)\")\n",
    "print(f\"Percentage > 300 tokens: {percent_over_300:.2f}% ({sum(l > 300 for l in non_zero_lengths)} sequences)\")\n",
    "print(f\"Percentage > 256 tokens: {percent_over_256:.2f}% ({sum(l > 256 for l in non_zero_lengths)} sequences)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
