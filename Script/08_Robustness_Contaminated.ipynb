{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b6bd260-c672-4df6-97bc-25c1f4ec8f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/carmine-\n",
      "[nltk_data]     landolfi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f866ae-a5c0-4c88-9b23-8f52138a1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/cleaned_dataset.csv\",encoding='utf-8')\n",
    "\n",
    "# p = df.groupby(\"primary_category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2709fa5e-2452-4f12-a3e6-a9a1adf0e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RobustnessConfig:\n",
    "    \"\"\"Configuration loaded from YAML for model and tokenizer.\"\"\"\n",
    "     \n",
    "    data_path: str\n",
    "    model_path: str\n",
    "    tokenizer_name: str\n",
    "    name_model: str\n",
    "    k_per_class: int\n",
    "    min_sent_len: int\n",
    "    n_sentences_per_insert: int\n",
    "    max_length: int\n",
    "    results_csv: str\n",
    "    batch_size: int\n",
    "    data_target_path: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b8f5f48-19d5-4da9-9dc6-346698ab8add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencePoolBuilder:\n",
    "    \"\"\"\n",
    "    Build a single global sentence pool from a dataframe using TF-IDF ranking.\n",
    "\n",
    "    The pool is used to contaminate datasets by sampling representative sentences.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_source : pd.DataFrame\n",
    "        Source dataframe used to extract sentences (this is your in-memory `df`).\n",
    "    text_col : str\n",
    "        Column name containing texts (e.g., \"abstract_clean\").\n",
    "    k_sentences : int\n",
    "        Number of top-ranked sentences (by TF-IDF) to keep globally.\n",
    "    min_sent_len : int\n",
    "        Minimum sentence length (in characters) to consider.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_source: pd.DataFrame,\n",
    "        text_col: str,\n",
    "        k_sentences: int = 2000,\n",
    "        min_sent_len: int = 20,\n",
    "    ) -> None:\n",
    "        self.df_source = df_source\n",
    "        self.text_col = text_col\n",
    "        self.k_sentences = k_sentences\n",
    "        self.min_sent_len = min_sent_len\n",
    "\n",
    "    def _split_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split a text into sentences and filter very short ones.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return []\n",
    "        sents = [s.strip() for s in sent_tokenize(text) if isinstance(s, str)]\n",
    "        return [s for s in sents if len(s) >= self.min_sent_len]\n",
    "\n",
    "    def build_pool(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Build a single global sentence pool using TF-IDF ranking.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            List of top-k sentences across the whole dataset.\n",
    "        \"\"\"\n",
    "        all_sentences: List[str] = []\n",
    "\n",
    "        for text in self.df_source[self.text_col].tolist():\n",
    "            sents = self._split_sentences(text)\n",
    "            all_sentences.extend(sents)\n",
    "\n",
    "        # Deduplicate\n",
    "        all_sentences = list(dict.fromkeys(all_sentences))\n",
    "\n",
    "        if not all_sentences:\n",
    "            return []\n",
    "\n",
    "        # TF-IDF over all sentences\n",
    "        tfidf = TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_df=0.9,\n",
    "            min_df=2,\n",
    "            max_features=50000,\n",
    "        )\n",
    "        X = tfidf.fit_transform(all_sentences)\n",
    "\n",
    "        # Sentence score: mean TF-IDF weight\n",
    "        scores = np.asarray(X.mean(axis=1)).ravel()\n",
    "\n",
    "        # Sort globally and keep top-k\n",
    "        sorted_idx = np.argsort(scores)[::-1]\n",
    "        top_idxs = sorted_idx[: self.k_sentences]\n",
    "        pool = [all_sentences[i] for i in top_idxs]\n",
    "\n",
    "        return pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6778f0a7-4f99-4ae8-9162-02b1c258f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractContaminator:\n",
    "    \"\"\"\n",
    "    Contaminate a target dataframe by inserting sentences sampled from a global pool.\n",
    "\n",
    "    The contamination is applied to a fraction alpha of rows (uniformly sampled).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_target: pd.DataFrame,\n",
    "        text_col: str,\n",
    "        pool: List[str],\n",
    "    ) -> None:\n",
    "        self.df_target = df_target.copy()\n",
    "        self.text_col = text_col\n",
    "        self.pool = pool\n",
    "        self.rng = random.Random(42)\n",
    "\n",
    "    def _insert_sentences_into_text(self, text: str, contam_sents: List[str]) -> str:\n",
    "        \"\"\"Insert contamination at the end of one random sentence within the text.\"\"\"\n",
    "        sents = [s.strip() for s in sent_tokenize(text)]\n",
    "        if not sents:\n",
    "            # If tokenization fails, just append at the end\n",
    "            return (text or \"\") + \" \" + \" \".join(contam_sents)\n",
    "\n",
    "        insert_idx = self.rng.randrange(0, len(sents))\n",
    "        sents[insert_idx] = (sents[insert_idx].rstrip() + \" \" + \" \".join(contam_sents)).strip()\n",
    "        return \" \".join(sents)\n",
    "\n",
    "    def contaminate(self, alpha: float, n_sentences: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply contamination to a fraction alpha of rows (0 < alpha <= 1).\n",
    "        \"\"\"\n",
    "        df_out = self.df_target.copy()\n",
    "        df_out[\"contaminated\"] = False\n",
    "\n",
    "        n = len(df_out)\n",
    "        k = max(1, int(round(alpha * n)))\n",
    "        candidate_indices = list(range(n))\n",
    "        self.rng.shuffle(candidate_indices)\n",
    "        to_contam = set(candidate_indices[:k])\n",
    "\n",
    "        for idx, row in df_out.iterrows():\n",
    "            if idx not in to_contam:\n",
    "                continue\n",
    "\n",
    "            if len(self.pool) == 0:\n",
    "                continue\n",
    "\n",
    "            k_sents = min(n_sentences, len(self.pool))\n",
    "            contam_sents = self.rng.sample(self.pool, k_sents)\n",
    "\n",
    "            text = row[self.text_col]\n",
    "            if not isinstance(text, str) or not text.strip():\n",
    "                continue\n",
    "\n",
    "            new_text = self._insert_sentences_into_text(text, contam_sents)\n",
    "            df_out.at[idx, self.text_col] = new_text\n",
    "            df_out.at[idx, \"contaminated\"] = True\n",
    "\n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd76d47-78ce-456b-9626-abe065b9bee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SimpleTextDataset(Dataset):\n",
    "    \"\"\"Torch dataset for batched encoding/prediction.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        labels: Optional[List[int]] = None,\n",
    "        tokenizer=None,\n",
    "        max_length: int = 300,\n",
    "    ) -> None:\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Load a fine-tuned HF model + tokenizer, run predictions, and compute metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str\n",
    "        Path to the fine-tuned model (HF format, directory with config + weights).\n",
    "    tokenizer_name : str\n",
    "        Name or path of the tokenizer to use.\n",
    "    device : Optional[str]\n",
    "        'cuda' or 'cpu'. If None, auto-detect.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str, tokenizer_name: str, device: Optional[str] = None) -> None:\n",
    "        \n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.device = torch.device(device)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
    "\n",
    "        # Try to get label2id from model config (preferable to match training)\n",
    "        self.label2id: Dict[str, int] = getattr(self.model.config, \"label2id\", {}) or {}\n",
    "        self.id2label: Dict[int, str] = getattr(self.model.config, \"id2label\", {}) or {}\n",
    "        # Normalize possible stringified integers\n",
    "        self.label2id = {str(k): int(v) for k, v in self.label2id.items()} if self.label2id else {}\n",
    "\n",
    "    def _encode_labels(\n",
    "        self, labels_str: List[str], strict: bool = True\n",
    "    ) -> Tuple[List[int], Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Map string labels to ids, using model config if available; else build a mapping.\n",
    "\n",
    "        If `strict=True` and some labels are missing in the model mapping, raise ValueError.\n",
    "        \"\"\"\n",
    "        if self.label2id:\n",
    "            missing = [l for l in set(labels_str) if str(l) not in self.label2id]\n",
    "            if missing and strict:\n",
    "                raise ValueError(\n",
    "                    f\"Found labels not present in model config label2id: {missing}. \"\n",
    "                    f\"Please ensure your evaluation set uses the same label space as training.\"\n",
    "                )\n",
    "            # Map known labels\n",
    "            label2id_eff = self.label2id.copy()\n",
    "            # Fallback for any stray label\n",
    "            next_id = max(label2id_eff.values()) + 1 if label2id_eff else 0\n",
    "            for l in set(labels_str):\n",
    "                if str(l) not in label2id_eff:\n",
    "                    label2id_eff[str(l)] = next_id\n",
    "                    next_id += 1\n",
    "        else:\n",
    "            # Build a mapping from the dataset (sorted for determinism)\n",
    "            uniq = sorted(set(labels_str))\n",
    "            label2id_eff = {str(l): i for i, l in enumerate(uniq)}\n",
    "\n",
    "        y = [label2id_eff[str(l)] for l in labels_str]\n",
    "        return y, label2id_eff\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        text_col: str,\n",
    "        label_col: str,\n",
    "        batch_size: int = 16,\n",
    "        max_length: int = 512,\n",
    "        strict_labels: bool = True,\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Tokenize, predict and compute loss/accuracy/f1/precision/recall.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, float]\n",
    "        \"\"\"\n",
    "        texts = df[text_col].astype(str).tolist()\n",
    "        labels_str = df[label_col].astype(str).tolist()\n",
    "        y_true, _ = self._encode_labels(labels_str, strict=strict_labels)\n",
    "\n",
    "        dataset = SimpleTextDataset(texts, y_true, tokenizer=self.tokenizer, max_length=max_length)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.model.eval()\n",
    "        loss_fn = CrossEntropyLoss()\n",
    "        total_loss = 0.0\n",
    "        n_examples = 0\n",
    "        preds_all: List[int] = []\n",
    "        trues_all: List[int] = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                logits = outputs.logits\n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "                total_loss += loss.item() * labels.size(0)\n",
    "                n_examples += labels.size(0)\n",
    "\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                preds_all.extend(preds.cpu().tolist())\n",
    "                trues_all.extend(labels.cpu().tolist())\n",
    "\n",
    "        avg_loss = total_loss / max(1, n_examples)\n",
    "        acc = accuracy_score(trues_all, preds_all)\n",
    "        f1 = f1_score(trues_all, preds_all, average=\"weighted\")\n",
    "        prec = precision_score(trues_all, preds_all, average=\"weighted\", zero_division=0)\n",
    "        rec = recall_score(trues_all, preds_all, average=\"weighted\")\n",
    "\n",
    "        return {\n",
    "            \"loss\": float(avg_loss),\n",
    "            \"accuracy\": float(acc),\n",
    "            \"f1\": float(f1),\n",
    "            \"precision\": float(prec),\n",
    "            \"recall\": float(rec),\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a12438b1-a7a9-4a2e-a859-cb4eafdfbc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: ../Datasets/robustness_contaminated_2_results.csv\n",
      "     model  alpha      loss  accuracy        f1  precision    recall\n",
      "0  SciBERT    0.0  1.030970  0.705846  0.698905   0.706064  0.705846\n",
      "1  SciBERT    0.1  1.051300  0.701206  0.694472   0.701992  0.701206\n",
      "2  SciBERT    0.2  1.081345  0.695639  0.689280   0.697970  0.695639\n",
      "3  SciBERT    0.3  1.103692  0.689040  0.683439   0.693542  0.689040\n"
     ]
    }
   ],
   "source": [
    "#### Initial parameters\n",
    "Config_yaml_path = \"../File_Yaml/Robustness_2.yaml\"  \n",
    "Alphas = [0.1, 0.2, 0.3]\n",
    "\n",
    "# 1) Load config\n",
    "with open(Config_yaml_path, \"r\") as f:\n",
    "    cfg_raw = yaml.safe_load(f)\n",
    "    \n",
    "cfg = RobustnessConfig(\n",
    "    data_path = cfg_raw[\"data_path\"],\n",
    "    model_path=cfg_raw[\"model_path\"],\n",
    "    tokenizer_name=cfg_raw[\"tokenizer_name\"],\n",
    "    name_model=cfg_raw[\"name_model\"],\n",
    "    k_per_class = cfg_raw[\"top_k\"],   # lo usiamo come numero globale di frasi\n",
    "    min_sent_len = cfg_raw[\"min_sent_len\"],\n",
    "    n_sentences_per_insert = cfg_raw[\"n_sentences_at_time\"],\n",
    "    max_length = cfg_raw[\"max_length\"],\n",
    "    results_csv=cfg_raw[\"results_csv\"],\n",
    "    batch_size = cfg_raw[\"batch_size\"],\n",
    "    data_target_path=cfg_raw[\"data_target_path\"],\n",
    ")\n",
    "\n",
    "# load dataset to create a pool of sentences\n",
    "df = pd.read_csv(cfg.data_path, encoding='utf-8')\n",
    "text_col = \"abstract_clean\"\n",
    "label_col = \"primary_category\"  \n",
    "\n",
    "# 2) Build TF-IDF global sentence pool\n",
    "pool_builder = SentencePoolBuilder(\n",
    "    df_source=df,\n",
    "    text_col=text_col,\n",
    "    k_sentences=cfg.k_per_class,\n",
    "    min_sent_len=cfg.min_sent_len,\n",
    ")\n",
    "sentence_pool = pool_builder.build_pool()\n",
    "\n",
    "# 3) Load the target dataset \n",
    "df_target = pd.read_csv(cfg.data_target_path)\n",
    "\n",
    "# Split train/val\n",
    "df_target_train, df_target_val  = train_test_split(\n",
    "      df_target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 4) Prepare contaminator and evaluator\n",
    "contaminator = AbstractContaminator(\n",
    "    df_target=df_target_val,\n",
    "    text_col=text_col,\n",
    "    pool=sentence_pool,\n",
    ")\n",
    "\n",
    "evaluator = ModelEvaluator(model_path=cfg.model_path, tokenizer_name=cfg.tokenizer_name)\n",
    "\n",
    "# 5) Loop over alphas, contaminate, evaluate, collect results\n",
    "results: List[Dict[str, float]] = []\n",
    "\n",
    "# Evaluate also the clean baseline (alpha = 0.0)\n",
    "metrics_clean = evaluator.evaluate(\n",
    "    df_target_val,\n",
    "    text_col=text_col,\n",
    "    label_col=label_col,\n",
    "    batch_size=cfg.batch_size,\n",
    "    max_length=cfg.max_length,\n",
    ")\n",
    "metrics_clean[\"model\"] = cfg.name_model\n",
    "metrics_clean[\"alpha\"] = 0.0\n",
    "results.append(metrics_clean)\n",
    "\n",
    "for alpha in Alphas:\n",
    "    df_cont = contaminator.contaminate(alpha=alpha, n_sentences=cfg.n_sentences_per_insert)\n",
    "    metrics = evaluator.evaluate(\n",
    "        df_cont,\n",
    "        text_col=text_col,\n",
    "        label_col=label_col,\n",
    "        batch_size=cfg.batch_size,\n",
    "        max_length=cfg.max_length,\n",
    "    )\n",
    "    metrics[\"model\"] = cfg.name_model\n",
    "    metrics[\"alpha\"] = float(alpha)\n",
    "    results.append(metrics)\n",
    "\n",
    "# 6) Save to CSV (append if exists)\n",
    "os.makedirs(os.path.dirname(cfg.results_csv), exist_ok=True)\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"model\", \"alpha\", \"loss\", \"accuracy\", \"f1\", \"precision\", \"recall\"]\n",
    ")\n",
    "\n",
    "if not os.path.isfile(cfg.results_csv):\n",
    "    results_df.to_csv(cfg.results_csv, index=False)\n",
    "else:\n",
    "    results_df.to_csv(cfg.results_csv, mode='a', header=False, index=False)\n",
    "\n",
    "print(f\"Results saved to: {cfg.results_csv}\")\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
